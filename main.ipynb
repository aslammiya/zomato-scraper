{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requiremnts\n",
    "%pip install selenium\n",
    "%pip install bs4\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source(url):\n",
    "    # Set up Chrome options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('log-level=3')\n",
    "    chrome_options.add_argument(\"--start-maximized\")\n",
    "    chrome_options.add_argument('--blink-settings=imagesEnabled=false')\n",
    "    chrome_options.add_argument('--disable-images')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    # Initialize the WebDriver\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    # URL of the page you want to scrape\n",
    "    url = url\n",
    "    driver.get(url)\n",
    "    html = \"\"  \n",
    "    try:\n",
    "        # Wait until all \"read more\" buttons are present\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        buttons = wait.until(EC.presence_of_all_elements_located((By.XPATH, \"//span[text()=' read more']\")))\n",
    "\n",
    "\n",
    "        # Click each button\n",
    "        for i, button in enumerate(buttons):\n",
    "            try:\n",
    "                # Scroll into view\n",
    "                print(f\"\\rExpanding all the {i+1}/{len(buttons)} descriptions...\", end=\"\")\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", button)\n",
    "                sleep(0.3)\n",
    "\n",
    "                # Ensure the element is clickable\n",
    "                wait.until(EC.element_to_be_clickable(button))\n",
    "\n",
    "                # Click using JavaScript if regular click fails\n",
    "                driver.execute_script(\"arguments[0].click();\", button)\n",
    "\n",
    "                # sleep(0.3)\n",
    "            except Exception as e:\n",
    "                print(f\"Error clicking button: {e}\")\n",
    "\n",
    "\n",
    "        # Get page source after clicking all buttons\n",
    "        html = driver.page_source if driver.page_source else \"\" \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "    # Save the page source to a file\n",
    "    with open('page_source.html', 'w', encoding='utf-8') as file:\n",
    "        file.write(html)\n",
    "\n",
    "    # Close the driver\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://www.zomato.com/mumbai/tru-falafel-khar/order\"\n",
    "url = \"https://www.zomato.com/mumbai/nothing-but-chicken-pali-hill-bandra-west/order\"\n",
    "# url = \"https://www.zomato.com/mumbai/dadar-social-dadar-west/order\"\n",
    "\n",
    "get_source(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"page_source.html\",encoding='utf-8') as html:\n",
    "    content = html.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_items = soup.find_all('div', {'type': ['veg', 'non-veg']})\n",
    "data = []  \n",
    "for elm in food_items:\n",
    "    # Determine the food type\n",
    "    food_type = elm.get('type')\n",
    "\n",
    "    # Find the sibling element\n",
    "    sibling = elm.find_parent().find_next_sibling()\n",
    "\n",
    "    if not sibling:\n",
    "        sibling = elm.find_next_sibling()\n",
    "\n",
    "    if sibling:\n",
    "        # Find the section tag by moving up the hierarchy from the sibling\n",
    "        section_tag = sibling.find_parent()\n",
    "        while section_tag and section_tag.name != 'section':\n",
    "            section_tag = section_tag.find_parent()\n",
    "\n",
    "        if section_tag:\n",
    "            # Extract the h4 tag text within the section\n",
    "            section = section_tag.find('h4')\n",
    "            if section:\n",
    "                section_text = section.get_text().strip()\n",
    "            else:\n",
    "                print(\"No section found within the section.\")\n",
    "        else:\n",
    "            print(\"No section tag found.\")\n",
    "\n",
    "        # Extract the sub headding\n",
    "        sub_heading = \"\"\n",
    "        if section_tag:\n",
    "            sub_hedding_find = section_tag.select('section > div > p')\n",
    "            if sub_heading:            \n",
    "                for p_tag in sub_hedding_find:\n",
    "                    sub_heading = p_tag.get_text().strip()\n",
    "            else:\n",
    "                sub_heading = \"No sub headding\"\n",
    "\n",
    "        # Extract the title\n",
    "        title_h = sibling.select_one(\"div > div > div > h4\")\n",
    "        title = title_h.get_text().strip() if title_h else None\n",
    "\n",
    "        # Extract the votes\n",
    "        vote_span = sibling.select_one(\"span:contains('votes')\")\n",
    "        vote = vote_span.get_text().strip() if vote_span else None\n",
    "\n",
    "        # Extract the price\n",
    "        price_span = sibling.select_one(\"div > div > div > div > span:not(:contains('votes'))\")\n",
    "        price = price_span.get_text().replace('â‚¹', '').strip() if price_span else None\n",
    "\n",
    "        # Extract the description\n",
    "        description_p = sibling.select_one(\"p\")\n",
    "        description = description_p.get_text().strip() if description_p else None\n",
    "\n",
    "        data.append({\n",
    "            'Title': title,\n",
    "            'Type': food_type,\n",
    "            'Price': price,\n",
    "            'Votes': vote,\n",
    "            'Description': description,\n",
    "            'Sub Headding': sub_heading,\n",
    "            'Section': section_text\n",
    "        })\n",
    "\n",
    "        # print(f\"Title: {title}\\nType: {food_type}\\nPrice: {price}\\nVotes: {vote}\\nDescription: {description}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "try:\n",
    "    df.to_csv('output.csv', index=False)\n",
    "    print(\"Output saved to output.csv\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
